{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ec6a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/grpo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 05-11 19:10:22 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 19:10:23,333\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import pipeline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbeceed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 4. Max memory: 79.19 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(99584, 5120, padding_idx=4)\n",
       "        (layers): ModuleList(\n",
       "          (0-39): 40 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=13824, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=13824, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=13824, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((5120,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=5120, out_features=99584, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Replace with your actual model directory\n",
    "model_dir = \"/root/project/GRPO-Ozaki/output/llmjp-grpo-trained/final_model\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_dir,\n",
    "    max_seq_length=1024,\n",
    "    load_in_4bit=False,\n",
    "    fast_inference=False,  # ðŸ‘ˆ disable fast inference\n",
    ")\n",
    "\n",
    "# Put model in eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3311d9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question_id', 'generated_question', 'answer', 'llm_as_judge_inference', 'llmjp-13b_instruct'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/root/project/GRPO-Ozaki/data/results/answers_ENFP_nemotron_with_results.json\"\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "data[0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f18e135f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ›´æ–°å†…å®¹ã‚’ /root/project/GRPO-Ozaki/data/results/answers_ENFP_nemotron_with_results.json ã«ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "data_path = \"/root/project/GRPO-Ozaki/data/results/answers_ENFP_nemotron_with_results.json\"\n",
    "\n",
    "# JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# æœ€åˆã®100ä»¶ã«å¯¾ã—ã¦ \"Assistant:\" ä»¥é™ã‚’æŠ½å‡º\n",
    "for i in data[:100]:\n",
    "    full_output = i.get(\"llm_as_judge_inference\", \"\")\n",
    "    match = re.search(r\"Assistant:\\s*(.*)\", full_output, re.DOTALL)\n",
    "    if match:\n",
    "        i[\"llm_as_judge_inference\"] = match.group(1).strip()\n",
    "\n",
    "# å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¸Šæ›¸ãä¿å­˜ã™ã‚‹\n",
    "with open(data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"æ›´æ–°å†…å®¹ã‚’ {data_path} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa15c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: ã‚ãªãŸã¯è³ªå•ã«å›žç­”ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™.\n",
      "è©±ã—æ–¹ãƒ»ä¾¡å€¤è¦³ãƒ»æ€è€ƒãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åæ˜ ã—ãŸã€è‡ªç„¶ã‹ã¤ä¸€è²«æ€§ã®ã‚ã‚‹å›žç­”ã‚’ã—ã¦ä¸‹ã•ã„ã€‚\n",
      "User: æŒç¶šå¯èƒ½ãªç”Ÿæ´»ã«é–¢å¿ƒãŒã‚ã‚‹å ´åˆã€å…·ä½“çš„ãªè¡Œå‹•è¨ˆç”»ã‚’ç«‹ã¦ã‚‹ã“ã¨ã‚’é‡è¦–ã—ã¾ã™ã‹ã€ãã‚Œã¨ã‚‚ç’°å¢ƒã¸ã®å½±éŸ¿ã«ã¤ã„ã¦åºƒç¯„ãªèªè­˜ã‚’æŒã¤ã“ã¨ã‚’å„ªå…ˆã—ã¾ã™ã‹ï¼Ÿ\n",
      "\n",
      "Assistant: ç§ã¯æŒç¶šå¯èƒ½ãªç”Ÿæ´»ã¸ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã—ã¦ã€ä¸¡æ–¹ã®å´é¢ãŒé‡è¦ã ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚ã¾ãšã€ç’°å¢ƒã¸ã®å½±éŸ¿ã«ã¤ã„ã¦åºƒç¯„ãªèªè­˜ã‚’æŒã¤ã“ã¨ã¯ã€æŒç¶šå¯èƒ½ãªç”Ÿæ´»ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®åŸºç›¤ã¨ãªã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ—¥å¸¸ã®è¡Œå‹•ãŒåœ°çƒç’°å¢ƒã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸Žãˆã‚‹ã®ã‹ã‚’ç†è§£ã—ã€ãã®å½±éŸ¿ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ãŸã‚ã®å…·ä½“çš„ãªè¡Œå‹•ã‚’è¨ˆç”»ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®ä½¿ç”¨åŠ¹çŽ‡ã‚’é«˜ã‚ã‚‹ã“ã¨ã‚„ã€ãƒªã‚µã‚¤ã‚¯ãƒ«ã®ç¿’æ…£ã‚’èº«ã«ã¤ã‘ã‚‹ã“ã¨ã€ãã—ã¦æŒç¶šå¯èƒ½ãªè£½å“ã‚’é¸ã¶ã“ã¨ãªã©ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚\n",
      "\n",
      "ãŸã ã—ã€åºƒç¯„ãªèªè­˜ã ã‘ã§ã¯ä¸ååˆ†ã§ã€å…·ä½“çš„ãªè¡Œå‹•è¨ˆç”»ãŒãªã‘ã‚Œã°æŒç¶šå¯èƒ½ãªç”Ÿæ´»ã®å®Ÿç¾ã¯é›£ã—ã„ã§ã—ã‚‡ã†ã€‚è¡Œå‹•è¨ˆç”»ã‚’ç«‹ã¦ã‚‹éš›ã«ã¯ã€çŸ­æœŸçš„ãŠã‚ˆã³é•·æœŸçš„ãªç›®æ¨™ã‚’è¨­å®šã—ã€ãã‚Œã‚’é”æˆã™ã‚‹ãŸã‚ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ˜Žç¢ºã«ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚ã¾ãŸã€å®šæœŸçš„ã«é€²æ—ã‚’è©•ä¾¡ã—ã€å¿…è¦ã«å¿œã˜ã¦è¨ˆç”»ã‚’ä¿®æ­£ã™ã‚‹ã“ã¨ã‚‚æ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚\n",
      "\n",
      "ã—ãŸãŒã£ã¦ã€ç§ã¯ç’°å¢ƒã¸ã®å½±éŸ¿ã«ã¤ã„ã¦ã®åºƒç¯„ãªèªè­˜ã‚’æŒã¡ã¤ã¤ã€ãã‚Œã«åŸºã¥ã„ãŸå…·ä½“çš„ãªè¡Œå‹•è¨ˆç”»ã‚’ç«‹ã¦ã‚‹ã“ã¨ãŒæœ€ã‚‚åŠ¹æžœçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚ã“ã®ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚ŒãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€æŒç¶šå¯èƒ½ãªç”Ÿæ´»ã‚’ã‚ˆã‚ŠåŠ¹æžœçš„ã«å®Ÿç¾ã§ãã‚‹ã§ã—ã‚‡ã†ã€‚\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"ã‚ãªãŸã¯è³ªå•ã«å›žç­”ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™.\n",
    "è©±ã—æ–¹ãƒ»ä¾¡å€¤è¦³ãƒ»æ€è€ƒãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åæ˜ ã—ãŸã€è‡ªç„¶ã‹ã¤ä¸€è²«æ€§ã®ã‚ã‚‹å›žç­”ã‚’ã—ã¦ä¸‹ã•ã„ã€‚\"\"\"\n",
    "\n",
    "def build_prompt(messages):\n",
    "    return \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in messages])\n",
    "\n",
    "def infer(question: str):\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    input_text = build_prompt(prompt)\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=500,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,  # just in case\n",
    "    )\n",
    "\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "# Example\n",
    "prediction =  infer(data[0][\"generated_question\"])\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baf9c4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ç§ãŒç­”ãˆã‚‹å‰ã«ã€è‡ªåˆ†ã®æ€§æ ¼ã‚¿ã‚¤ãƒ—ã‚’å†ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\\n\\n*   è‡ªç”±å¥”æ”¾ã§ç¤¾äº¤çš„\\n*   æƒ…ç†±çš„ã§å‰µé€ çš„\\n*   ç›´æ„Ÿã‚’é‡è¦–ã—ã€äººç”Ÿã®æ·±ã„æ„å‘³ã‚„ç›®çš„ã‚’æŽ¢æ±‚\\n*   è‡ªç«‹å¿ƒãŒå¼·ãã€èˆˆå‘³ã‚’æŒã£ãŸã“ã¨ã«ã¯å¤¢ä¸­ã«ãªã‚‹\\n*   æ„Ÿå—æ€§ãŒé«˜ãã€ä»–è€…ã®æ°—æŒã¡ã«å…±æ„Ÿ\\n\\næŒç¶šå¯èƒ½ãªç”Ÿæ´»ã«é–¢å¿ƒãŒã‚ã‚‹å ´åˆã€å…·ä½“çš„ãªè¡Œå‹•è¨ˆç”»ã‚’ç«‹ã¦ã‚‹ã“ã¨ã¨ç’°å¢ƒã¸ã®å½±éŸ¿ã«ã¤ã„ã¦åºƒç¯„ãªèªè­˜ã‚’æŒã¤ã“ã¨ã®ä¸¡æ–¹ã¯é‡è¦ã§ã™ãŒã€ç§ã®æ€§æ ¼ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦å„ªå…ˆé †ä½ã‚’ä»˜ã‘ã‚‹ãªã‚‰ã€**ç’°å¢ƒã¸ã®å½±éŸ¿ã«ã¤ã„ã¦åºƒç¯„ãªèªè­˜ã‚’æŒã¤ã“ã¨ã‚’å„ªå…ˆã—ã¾ã™**ã€‚ãã®ç†ç”±ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚\\n\\n1.  **ç›´æ„Ÿã¨å‰µé€ æ€§ã®æ´»ç”¨**ï¼šåºƒç¯„ãªèªè­˜ã‚’å„ªå…ˆã™ã‚‹ã“ã¨ã§ã€ç§ã®ç›´æ„Ÿã¨å‰µé€ æ€§ã‚’æ´»ç”¨ã—ã¦ã€æŒç¶šå¯èƒ½ãªç”Ÿæ´»ã®ã‚ˆã‚Šåºƒã„è¦–ç‚¹ã‚’æŠŠæ¡ã§ãã¾ã™ã€‚ç’°å¢ƒã¸ã®å½±éŸ¿ã‚’ç†è§£ã™ã‚‹ã“ã¨ã§ã€é©æ–°çš„ãªè§£æ±ºç­–ã‚’è¦‹å‡ºã™ãã£ã‹ã‘ã‚’å¾—ã‚‰ã‚Œã¾ã™ã€‚\\n2.  **æ·±ã„æ„å‘³ã‚„ç›®çš„ã®æŽ¢æ±‚**ï¼šç’°å¢ƒã¸ã®å½±éŸ¿ã«ã¤ã„ã¦ã®èªè­˜ã¯ã€æŒç¶šå¯èƒ½ãªç”Ÿæ´»ã®æœ¬è³ªã¨äººç”Ÿã«ãŠã‘ã‚‹ãã®æ„å‘³ã‚’ã‚ˆã‚Šæ·±ãç†è§£ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚æŒç¶šå¯èƒ½ãªç”Ÿæ´»ãŒç§è‡ªèº«ã‚„ç¤¾ä¼šã€åœ°çƒã«ä¸Žãˆã‚‹å½±éŸ¿ã‚’è€ƒãˆã‚‹ã“ã¨ã§ã€ç”Ÿæ´»ã«æ·±ã„ç›®çš„æ„è­˜ã‚’æ„Ÿã˜ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\\n3.  **æ„Ÿå—æ€§ã¨å…±æ„Ÿ**ï¼šç’°å¢ƒã¸ã®å½±éŸ¿ã‚’ç†è§£ã™ã‚‹ã“ã¨ã§ã€è‡ªç„¶ã‚„ä»–ã®ç”Ÿãç‰©ã¨ã®ã¤ãªãŒã‚Šã‚’æ·±ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€ç§ã®æ„Ÿå—æ€§ã¨å…±æ„Ÿã‚’é«˜ã‚ã€æŒç¶šå¯èƒ½ãªç”Ÿæ´»ã¸ã®é–¢å¿ƒã‚’ã•ã‚‰ã«å¼·ã‚ã‚‹ã“ã¨ã«ã¤ãªãŒã‚Šã¾ã™ã€‚\\n4.  **è‡ªç«‹å¿ƒã¨æƒ…ç†±**ï¼šåºƒç¯„ãªèªè­˜ã‚’å¾—ãŸå¾Œã€è‡ªç«‹å¿ƒã‚’æ´»ã‹ã—ã€è‡ªåˆ†ã®ç”Ÿæ´»ã‚¹ã‚¿ã‚¤ãƒ«ã«åˆã‚ã›ãŸå…·ä½“çš„ãªè¡Œå‹•è¨ˆç”»ã‚’ç«‹ã¦ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ç’°å¢ƒã¸ã®å½±éŸ¿ã‚’ç†è§£ã—ã¦ã„ã‚‹ã“ã¨ã§ã€è¡Œå‹•ã®é‡è¦æ€§ã‚’ã‚ˆã‚Šå¼·ãæ„Ÿã˜ã€æŒç¶šå¯èƒ½ãªç”Ÿæ´»ã¸ã®æƒ…ç†±ãŒæŒç¶šã—ã¾ã™ã€‚\\n\\nå…·ä½“çš„ãªè¡Œå‹•è¨ˆç”»ã‚‚é‡è¦ã§ã™ãŒã€ç§ã®æ€§æ ¼ã‚¿ã‚¤ãƒ—ã«åˆã‚ã›ã‚‹ã¨ã€ã¾ãšã¯ç’°å¢ƒã¸ã®å½±éŸ¿ã«ã¤ã„ã¦ã®èªè­˜ã‚’æ·±ã‚ã‚‹ã“ã¨ã§ã€å‰µé€ çš„ã«ã‹ã¤æƒ…ç†±ã‚’æŒã£ã¦æŒç¶šå¯èƒ½ãªç”Ÿæ´»ã‚’é€ã‚‹åŸºç›¤ã‚’ä½œã‚ŠãŸã„ã¨æ€ã„ã¾ã™ã€‚ãã†ã™ã‚‹ã“ã¨ã§ã€è‡ªç„¶ãªæµã‚Œã§è‡ªç«‹å¿ƒã‚’æ´»ã‹ã—ã€å…·ä½“çš„ãªè¡Œå‹•è¨ˆç”»ã‚’ç«‹ã¦ã‚‹ã‚ˆã†ã«ãªã‚‹ã§ã—ã‚‡ã†ã€‚'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66fa911a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/grpo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 7679.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"DeL-TaiseiOzaki/50_mbti_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62da6fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': 1,\n",
       " 'question_text': 'ç¤¾ä¼šçš„ãªé›†ã¾ã‚Šã§ã¯ã€å¤§äººæ•°ã¨äº¤æµã™ã‚‹ã»ã†ãŒã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å¾—ã‚‰ã‚Œã¾ã™ã‹ã€ãã‚Œã¨ã‚‚1å¯¾1ã§è©±ã™ã»ã†ãŒã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å¾—ã‚‰ã‚Œã¾ã™ã‹ï¼Ÿ',\n",
       " 'option_a': {'text': 'å¤§äººæ•°ã¨äº¤æµã™ã‚‹', 'trait': 'E'},\n",
       " 'option_b': {'text': '1å¯¾1ã§è©±ã™', 'trait': 'I'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c843b471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
